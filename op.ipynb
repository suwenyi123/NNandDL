{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5712bd7-49a3-4fbc-a32c-52fbb1226b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # 防止数值溢出\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    N = y_true.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / N  # 加上 1e-8 防止 log(0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95ff69e-a44e-4ae5-8197-6cdf2a1ecedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (45000, 3072)\n",
      "y_train shape: (45000, 10)\n",
      "X_val shape: (5000, 3072)\n",
      "y_val shape: (5000, 10)\n",
      "X_test shape: (10000, 3072)\n",
      "y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 定义 unpickle 函数\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data\n",
    "\n",
    "# 加载 CIFAR-10 数据集\n",
    "def load_cifar10_from_local(data_dir):\n",
    "\n",
    "    # 加载训练数据\n",
    "    X_train, y_train = [], []\n",
    "    for i in range(1, 6):  # CIFAR-10 有 5 个训练 batch\n",
    "        file_path = os.path.join(data_dir, f'data_batch_{i}')\n",
    "        data = unpickle(file_path)\n",
    "        X_train.append(data[b'data'])       # 图像数据\n",
    "        y_train.extend(data[b'labels'])    # 标签\n",
    "\n",
    "    # 合并训练数据\n",
    "    X_train = np.vstack(X_train)           # (50000, 3072)\n",
    "    y_train = np.array(y_train)            # (50000,)\n",
    "\n",
    "    # 加载测试数据\n",
    "    test_file_path = os.path.join(data_dir, 'test_batch')\n",
    "    test_data = unpickle(test_file_path)\n",
    "    X_test = test_data[b'data']            # (10000, 3072)\n",
    "    y_test = np.array(test_data[b'labels'])  # (10000,)\n",
    "\n",
    "    # 归一化到 [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "    # 展平图像\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)  # (50000, 3072)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)     # (10000, 3072)\n",
    "\n",
    "    # 将标签转换为 one-hot 编码\n",
    "    encoder = OneHotEncoder(sparse_output=False)  \n",
    "    y_train = encoder.fit_transform(y_train.reshape(-1, 1))  # (50000, 10)\n",
    "    y_test = encoder.transform(y_test.reshape(-1, 1))        # (10000, 10)\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# 指定 CIFAR-10 数据集的本地路径\n",
    "data_dir = r\"E:\\NNandDL\\first\\dataset\\cifar-10-python\\cifar-10-batches-py\"\n",
    "\n",
    "# 加载数据\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10_from_local(data_dir)\n",
    "\n",
    "# 打印数据形状以确认加载成功\n",
    "print(\"X_train shape:\", X_train.shape)  # 应为 (45000, 3072)\n",
    "print(\"y_train shape:\", y_train.shape)  # 应为 (45000, 10)\n",
    "print(\"X_val shape:\", X_val.shape)      # 应为 (5000, 3072)\n",
    "print(\"y_val shape:\", y_val.shape)      # 应为 (5000, 10)\n",
    "print(\"X_test shape:\", X_test.shape)    # 应为 (10000, 3072)\n",
    "print(\"y_test shape:\", y_test.shape)    # 应为 (10000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffcf8264-e6d7-4ad2-8025-b96feaec8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数范围\n",
    "learning_rates = [0.05]\n",
    "hidden_sizes_list = [[512]]\n",
    "l2_lambdas = [0.001]\n",
    "initializers = [\"xavier\"]\n",
    "optimizers = [\"momentum\"]\n",
    "lr_decay_types = [\"linear\"]\n",
    "decay_rates = [0.90]\n",
    "decay_steps_list = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b2ba6c-4c78-4bba-8140-632c0941952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle  # 用于保存和加载模型\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    N = y_true.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[np.arange(N), np.argmax(y_true, axis=1)] + 1e-8)\n",
    "    loss = np.sum(log_likelihood) / N\n",
    "    return loss\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, initializer=\"random\", optimizer=\"sgd\", learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # 初始化权重和偏置\n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            if initializer == \"random\":\n",
    "                # 随机初始化\n",
    "                self.weights.append(np.random.randn(sizes[i], sizes[i + 1]) * 0.01)\n",
    "            elif initializer == \"xavier\":\n",
    "                # Xavier 初始化\n",
    "                scale = np.sqrt(2 / (sizes[i] + sizes[i + 1]))\n",
    "                self.weights.append(np.random.randn(sizes[i], sizes[i + 1]) * scale)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported initializer: {initializer}\")\n",
    "            self.biases.append(np.zeros((1, sizes[i + 1])))\n",
    "        \n",
    "        # 初始化优化器相关变量\n",
    "        if optimizer == \"momentum\":\n",
    "            self.momentum_weights = [np.zeros_like(w) for w in self.weights]\n",
    "            self.momentum_biases = [np.zeros_like(b) for b in self.biases]\n",
    "            self.beta = 0.9  # 动量系数\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.layers = [X]\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            Z = np.dot(self.layers[-1], self.weights[i]) + self.biases[i]\n",
    "            A = relu(Z)\n",
    "            self.layers.append(A)\n",
    "        Z = np.dot(self.layers[-1], self.weights[-1]) + self.biases[-1]\n",
    "        Y_pred = softmax(Z)\n",
    "        self.layers.append(Y_pred)\n",
    "        return Y_pred\n",
    "\n",
    "    def backward(self, X, y_true, learning_rate, l2_lambda):\n",
    "        N = X.shape[0]\n",
    "        layers = self.layers\n",
    "        dA = layers[-1] - y_true\n",
    "        dA /= N\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            dZ = dA * (layers[i + 1] > 0)  # ReLU 激活函数的导数\n",
    "            dW = np.dot(layers[i].T, dZ)\n",
    "            db = np.sum(dZ, axis=0, keepdims=True)\n",
    "            dA = np.dot(dZ, self.weights[i].T)\n",
    "\n",
    "            # 添加 L2 正则化\n",
    "            dW += l2_lambda * self.weights[i]\n",
    "\n",
    "            # 根据优化器更新参数\n",
    "            if self.optimizer == \"sgd\":\n",
    "                self.weights[i] -= learning_rate * dW\n",
    "                self.biases[i] -= learning_rate * db\n",
    "            elif self.optimizer == \"momentum\":\n",
    "                self.momentum_weights[i] = self.beta * self.momentum_weights[i] + (1 - self.beta) * dW\n",
    "                self.momentum_biases[i] = self.beta * self.momentum_biases[i] + (1 - self.beta) * db\n",
    "                self.weights[i] -= learning_rate * self.momentum_weights[i]\n",
    "                self.biases[i] -= learning_rate * self.momentum_biases[i]\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs, batch_size, initial_lr, l2_lambda, lr_decay_type=\"constant\", decay_rate=0.96, decay_steps=10, save_path=None):\n",
    "        train_losses, val_losses, val_accuracies = [], [], []\n",
    "        best_val_loss = float('inf')\n",
    "        best_weights = None\n",
    "        best_biases = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # 根据学习率下降策略调整学习率\n",
    "            if lr_decay_type == \"exponential\":\n",
    "                learning_rate = initial_lr * (decay_rate ** (epoch / decay_steps))\n",
    "            elif lr_decay_type == \"linear\":\n",
    "                learning_rate = initial_lr * (1 - epoch / epochs)\n",
    "            elif lr_decay_type == \"constant\":\n",
    "                learning_rate = initial_lr\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported learning rate schedule: {lr_decay_type}\")\n",
    "\n",
    "            # 打乱数据\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            # 小批量梯度下降\n",
    "            for i in range(0, X_train.shape[0], batch_size):\n",
    "                X_batch = X_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "                self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch, learning_rate, l2_lambda)\n",
    "\n",
    "            # 计算训练和验证指标\n",
    "            train_loss = cross_entropy_loss(self.forward(X_train), y_train)\n",
    "            val_loss = cross_entropy_loss(self.forward(X_val), y_val)\n",
    "            val_accuracy = np.mean(np.argmax(self.forward(X_val), axis=1) == np.argmax(y_val, axis=1))\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Learning Rate: {learning_rate:.6f}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            # 保存最优模型权重\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_weights = [w.copy() for w in self.weights]\n",
    "                best_biases = [b.copy() for b in self.biases]\n",
    "                if save_path:\n",
    "                    self.save_model(save_path)  # 保存最优模型到指定路径\n",
    "\n",
    "        # 加载最优模型权重\n",
    "        self.weights = best_weights\n",
    "        self.biases = best_biases\n",
    "\n",
    "        return train_losses, val_losses, val_accuracies\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.forward(X_test)\n",
    "        accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_test, axis=1))\n",
    "        return accuracy\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        \"\"\"将模型的权重和偏置保存到本地\"\"\"\n",
    "        model_data = {\n",
    "            'weights': self.weights,\n",
    "            'biases': self.biases\n",
    "        }\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        \"\"\"从本地文件加载模型的权重和偏置\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            self.weights = model_data['weights']\n",
    "            self.biases = model_data['biases']\n",
    "            print(f\"Model loaded from {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File {file_path} not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba660f4b-16d3-40bb-baff-91afcaa3b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.05, hidden_sizes=[512], l2_lambda=0.001, initializer=xavier, optimizer=momentum, lr_decay_type=linear, decay_rate=0.9, decay_steps=10\n",
      "Epoch 1/40, Learning Rate: 0.050000, Train Loss: 1.7104, Val Loss: 1.7434, Val Accuracy: 0.3750\n",
      "Epoch 2/40, Learning Rate: 0.048750, Train Loss: 1.6076, Val Loss: 1.6485, Val Accuracy: 0.4154\n",
      "Epoch 3/40, Learning Rate: 0.047500, Train Loss: 1.5199, Val Loss: 1.5764, Val Accuracy: 0.4406\n",
      "Epoch 4/40, Learning Rate: 0.046250, Train Loss: 1.5217, Val Loss: 1.5874, Val Accuracy: 0.4320\n",
      "Epoch 5/40, Learning Rate: 0.045000, Train Loss: 1.5395, Val Loss: 1.6141, Val Accuracy: 0.4160\n",
      "Epoch 6/40, Learning Rate: 0.043750, Train Loss: 1.5216, Val Loss: 1.6098, Val Accuracy: 0.4282\n",
      "Epoch 7/40, Learning Rate: 0.042500, Train Loss: 1.3797, Val Loss: 1.4744, Val Accuracy: 0.4830\n",
      "Epoch 8/40, Learning Rate: 0.041250, Train Loss: 1.4124, Val Loss: 1.5188, Val Accuracy: 0.4534\n",
      "Epoch 9/40, Learning Rate: 0.040000, Train Loss: 1.3977, Val Loss: 1.5217, Val Accuracy: 0.4676\n",
      "Epoch 10/40, Learning Rate: 0.038750, Train Loss: 1.3180, Val Loss: 1.4370, Val Accuracy: 0.4890\n",
      "Epoch 11/40, Learning Rate: 0.037500, Train Loss: 1.2910, Val Loss: 1.4249, Val Accuracy: 0.4994\n",
      "Epoch 12/40, Learning Rate: 0.036250, Train Loss: 1.2744, Val Loss: 1.4176, Val Accuracy: 0.5012\n",
      "Epoch 13/40, Learning Rate: 0.035000, Train Loss: 1.3055, Val Loss: 1.4501, Val Accuracy: 0.4884\n",
      "Epoch 14/40, Learning Rate: 0.033750, Train Loss: 1.2558, Val Loss: 1.4203, Val Accuracy: 0.4982\n",
      "Epoch 15/40, Learning Rate: 0.032500, Train Loss: 1.2435, Val Loss: 1.4108, Val Accuracy: 0.4924\n",
      "Epoch 16/40, Learning Rate: 0.031250, Train Loss: 1.2223, Val Loss: 1.4070, Val Accuracy: 0.4988\n",
      "Epoch 17/40, Learning Rate: 0.030000, Train Loss: 1.2019, Val Loss: 1.3874, Val Accuracy: 0.5122\n",
      "Epoch 18/40, Learning Rate: 0.028750, Train Loss: 1.2110, Val Loss: 1.4015, Val Accuracy: 0.4986\n",
      "Epoch 19/40, Learning Rate: 0.027500, Train Loss: 1.1808, Val Loss: 1.3891, Val Accuracy: 0.5118\n",
      "Epoch 20/40, Learning Rate: 0.026250, Train Loss: 1.1818, Val Loss: 1.3854, Val Accuracy: 0.5108\n",
      "Epoch 21/40, Learning Rate: 0.025000, Train Loss: 1.1428, Val Loss: 1.3623, Val Accuracy: 0.5204\n",
      "Epoch 22/40, Learning Rate: 0.023750, Train Loss: 1.1717, Val Loss: 1.3842, Val Accuracy: 0.5068\n",
      "Epoch 23/40, Learning Rate: 0.022500, Train Loss: 1.1335, Val Loss: 1.3708, Val Accuracy: 0.5148\n",
      "Epoch 24/40, Learning Rate: 0.021250, Train Loss: 1.1422, Val Loss: 1.3877, Val Accuracy: 0.5074\n",
      "Epoch 25/40, Learning Rate: 0.020000, Train Loss: 1.0890, Val Loss: 1.3445, Val Accuracy: 0.5288\n",
      "Epoch 26/40, Learning Rate: 0.018750, Train Loss: 1.1051, Val Loss: 1.3652, Val Accuracy: 0.5232\n",
      "Epoch 27/40, Learning Rate: 0.017500, Train Loss: 1.0966, Val Loss: 1.3599, Val Accuracy: 0.5284\n",
      "Epoch 28/40, Learning Rate: 0.016250, Train Loss: 1.0460, Val Loss: 1.3140, Val Accuracy: 0.5354\n",
      "Epoch 29/40, Learning Rate: 0.015000, Train Loss: 1.0384, Val Loss: 1.3221, Val Accuracy: 0.5396\n",
      "Epoch 30/40, Learning Rate: 0.013750, Train Loss: 1.0435, Val Loss: 1.3280, Val Accuracy: 0.5352\n",
      "Epoch 31/40, Learning Rate: 0.012500, Train Loss: 1.0266, Val Loss: 1.3171, Val Accuracy: 0.5348\n",
      "Epoch 32/40, Learning Rate: 0.011250, Train Loss: 1.0333, Val Loss: 1.3257, Val Accuracy: 0.5336\n",
      "Epoch 33/40, Learning Rate: 0.010000, Train Loss: 1.0150, Val Loss: 1.3118, Val Accuracy: 0.5374\n",
      "Epoch 34/40, Learning Rate: 0.008750, Train Loss: 0.9926, Val Loss: 1.2977, Val Accuracy: 0.5456\n",
      "Epoch 35/40, Learning Rate: 0.007500, Train Loss: 0.9910, Val Loss: 1.3036, Val Accuracy: 0.5406\n",
      "Epoch 36/40, Learning Rate: 0.006250, Train Loss: 0.9832, Val Loss: 1.2974, Val Accuracy: 0.5446\n",
      "Epoch 37/40, Learning Rate: 0.005000, Train Loss: 0.9755, Val Loss: 1.2889, Val Accuracy: 0.5470\n",
      "Epoch 38/40, Learning Rate: 0.003750, Train Loss: 0.9666, Val Loss: 1.2924, Val Accuracy: 0.5440\n",
      "Epoch 39/40, Learning Rate: 0.002500, Train Loss: 0.9597, Val Loss: 1.2849, Val Accuracy: 0.5496\n",
      "Epoch 40/40, Learning Rate: 0.001250, Train Loss: 0.9565, Val Loss: 1.2850, Val Accuracy: 0.5520\n",
      "Test Accuracy: 0.5560\n",
      "\n",
      "\n",
      "Best Hyperparameters:\n",
      "learning_rate                                                 0.05\n",
      "hidden_sizes                                                 [512]\n",
      "l2_lambda                                                    0.001\n",
      "initializer                                                 xavier\n",
      "optimizer                                                 momentum\n",
      "lr_decay_type                                               linear\n",
      "decay_rate                                                     0.9\n",
      "decay_steps                                                     10\n",
      "val_accuracy                                                 0.552\n",
      "test_accuracy                                                0.556\n",
      "model_weights    [[[0.0019524557183629393, 0.000295419543241789...\n",
      "model_biases     [[[0.12647393376140145, 0.02811036931434421, 0...\n",
      "Name: 0, dtype: object\n",
      "Model saved to op.pkl\n",
      "\n",
      "Best model saved to op.pkl\n",
      "Results saved to 'op.csv'\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# 存储结果\n",
    "results = []\n",
    "\n",
    "# 遍历所有超参数组合\n",
    "for lr, hidden_sizes, l2_lambda, initializer, optimizer, lr_decay_type, decay_rate, decay_steps in itertools.product(\n",
    "    learning_rates, hidden_sizes_list, l2_lambdas, initializers, optimizers, lr_decay_types, decay_rates, decay_steps_list\n",
    "):\n",
    "    print(f\"Training with lr={lr}, hidden_sizes={hidden_sizes}, l2_lambda={l2_lambda}, initializer={initializer}, \"\n",
    "          f\"optimizer={optimizer}, lr_decay_type={lr_decay_type}, decay_rate={decay_rate}, decay_steps={decay_steps}\")\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = NeuralNetwork(input_size=3072, hidden_sizes=hidden_sizes, output_size=10,\n",
    "                          initializer=initializer, optimizer=optimizer, learning_rate=lr)\n",
    "    \n",
    "    # 训练模型（不再传递 save_path 参数）\n",
    "    train_losses, val_losses, val_accuracies = model.train(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        epochs=40, batch_size=64, initial_lr=lr, l2_lambda=l2_lambda,\n",
    "        lr_decay_type=lr_decay_type, decay_rate=decay_rate, decay_steps=decay_steps\n",
    "    )\n",
    "    \n",
    "    # 测试模型\n",
    "    test_accuracy = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # 记录结果\n",
    "    results.append({\n",
    "        \"learning_rate\": lr,\n",
    "        \"hidden_sizes\": hidden_sizes,\n",
    "        \"l2_lambda\": l2_lambda,\n",
    "        \"initializer\": initializer,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr_decay_type\": lr_decay_type,\n",
    "        \"decay_rate\": decay_rate,\n",
    "        \"decay_steps\": decay_steps,\n",
    "        \"val_accuracy\": val_accuracies[-1],  # 最后一个 epoch 的验证集准确率\n",
    "        \"test_accuracy\": test_accuracy,      # 测试集的准确率\n",
    "        \"model_weights\": [w.copy() for w in model.weights],  # 保存模型权重\n",
    "        \"model_biases\": [b.copy() for b in model.biases]     # 保存模型偏置\n",
    "    })\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "\n",
    "# 将结果转换为 DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 找出测试集上准确率最高的模型\n",
    "best_result = results_df.loc[results_df['test_accuracy'].idxmax()]\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_result)\n",
    "\n",
    "# 保存测试集上准确率最高的模型\n",
    "best_model = NeuralNetwork(input_size=3072, hidden_sizes=best_result[\"hidden_sizes\"], output_size=10,\n",
    "                           initializer=best_result[\"initializer\"], optimizer=best_result[\"optimizer\"],\n",
    "                           learning_rate=best_result[\"learning_rate\"])\n",
    "best_model.weights = best_result[\"model_weights\"]\n",
    "best_model.biases = best_result[\"model_biases\"]\n",
    "\n",
    "# 定义保存路径\n",
    "best_model_path = \"op.pkl\"\n",
    "best_model.save_model(best_model_path)\n",
    "print(f\"\\nBest model saved to {best_model_path}\")\n",
    "\n",
    "# 将结果保存为 CSV 文件\n",
    "results_df.to_csv(\"op.csv\", index=False)\n",
    "print(\"Results saved to 'op.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNandDL",
   "language": "python",
   "name": "nnanddl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
